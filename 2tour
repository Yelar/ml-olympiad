# üß† ML –û–ª–∏–º–ø–∏–∞–¥–∞: –ü–æ–ª–Ω—ã–π –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ø–æ –ë–∞–∑–æ–≤—ã–º –ú–µ—Ç–æ–¥–∞–º

## üî¨ –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –ø–æ –∑–∞–¥–∞—á–∞–º

### üì∑ –ó–∞–¥–∞—á–∞: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
**–ú–µ—Ç–æ–¥:** PCA –Ω–∞ flattened array –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
```python
X_images = images.reshape(images.shape[0], -1)
X_pca = PCA(n_components=50).fit_transform(X_images)
```

### üìö –ó–∞–¥–∞—á–∞: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏
**–ú–µ—Ç–æ–¥:** TF-IDF/CountVectorizer + LogisticRegression
```python
texts = df["sentence"]
tfidf = TfidfVectorizer(max_features=500)
X = tfidf.fit_transform(texts)
model = LogisticRegression()
model.fit(X, y)
```

### üìà –ó–∞–¥–∞—á–∞: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
**–ú–µ—Ç–æ–¥:** groupby + –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –¥–∞—Ç–µ + LGBMRegressor
```python
df["date"] = pd.to_datetime(df["date"])
df["month"] = df["date"].dt.month
df["year"] = df["date"].dt.year

agg = df.groupby("category")["num_papers"].agg(["mean", "std"])
df = df.merge(agg, on="category", how="left")

X = df[["month", "year", "mean", "std"]]
y = df["num_papers"]

model = lgb.LGBMRegressor()
model.fit(X, y)
```

### üê∂ –ó–∞–¥–∞—á–∞: –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ—Ä–≥–∏ (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, PCA)
**–ú–µ—Ç–æ–¥:** PCA + –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```python
X_corgis = images.reshape(images.shape[0], -1)
X_pca = PCA(n_components=2).fit_transform(X_corgis)
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title("Corgi Clusters")
```

### üîÑ –ó–∞–¥–∞—á–∞: –ø–æ–≤–æ—Ä–æ—Ç MNIST
**–ú–µ—Ç–æ–¥:** PCA –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö + –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, GradientBoosting)
```python
X_mnist = images.reshape(images.shape[0], -1)
X_pca = PCA(n_components=40).fit_transform(X_mnist)
model = GradientBoostingClassifier()
model.fit(X_pca, y)
```

### üí¨ –ó–∞–¥–∞—á–∞: –º–∞—Ç—á–∏–Ω–≥ –≤—Ä–∞—á–µ–±–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –∏ —Å–∞–º–º–∞—Ä–∏
**–ú–µ—Ç–æ–¥:** TF-IDF + cosine similarity
```python
from sklearn.metrics.pairwise import cosine_similarity

vec = TfidfVectorizer(max_features=500)
X = vec.fit_transform(df["report"])
Y = vec.transform(df["summary"])

similarities = cosine_similarity(X, Y)
preds = similarities.argmax(axis=1)
```

_–î–∞–ª–µ–µ ‚Äî –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ —Å–Ω–∏–ø–ø–µ—Ç—ã:_

## üß© –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

### 1. TF-IDF (–¥–ª—è —Ç–µ–∫—Å—Ç–∞)
```python
from sklearn.feature_extraction.text import TfidfVectorizer

texts = ["AI is the future", "Machine learning is a subset of AI"]
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(texts)
```

### 2. CountVectorizer (–ø—Ä–æ—Å—Ç–æ–π bag-of-words)
```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)
```

### 3. PCA (—Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫, TF-IDF –∏ —Ç.–ø.)
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())
```

### 4. GroupBy –≤ pandas (–¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
```python
import pandas as pd

df = pd.DataFrame({
    "category": ["cs", "cs", "physics"],
    "length": [100, 150, 200]
})
grouped = df.groupby("category")["length"].agg(["mean", "std"])
df = df.merge(grouped, on="category", how="left")
```

### 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (flatten + PCA)
```python
import numpy as np
X_images = images.reshape(images.shape[0], -1)  # (n_samples, height*width)
X_pca = PCA(n_components=50).fit_transform(X_images)
```

... (–æ—Å—Ç–∞–≤—à–∞—è—Å—è —á–∞—Å—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
