# üß† ML –û–ª–∏–º–ø–∏–∞–¥–∞: –ü–æ–ª–Ω—ã–π –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –ø–æ –ë–∞–∑–æ–≤—ã–º –ú–µ—Ç–æ–¥–∞–º

## üî¨ –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –ø–æ –∑–∞–¥–∞—á–∞–º

### üì∑ –ó–∞–¥–∞—á–∞: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
**–ú–µ—Ç–æ–¥:** PCA –Ω–∞ flattened array –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
```python
from sklearn.decomposition import PCA

X_images = images.reshape(images.shape[0], -1)
X_pca = PCA(n_components=50).fit_transform(X_images)
```

### üìö –ó–∞–¥–∞—á–∞: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏
**–ú–µ—Ç–æ–¥:** TF-IDF/CountVectorizer + LogisticRegression
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

texts = df["sentence"]
tfidf = TfidfVectorizer(max_features=500)
X = tfidf.fit_transform(texts)
model = LogisticRegression()
model.fit(X, y)
```

### üìà –ó–∞–¥–∞—á–∞: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
**–ú–µ—Ç–æ–¥:** groupby + –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –¥–∞—Ç–µ + LGBMRegressor
```python
import pandas as pd
import lightgbm as lgb

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –¥–∞—Ç–µ
df["date"] = pd.to_datetime(df["date"])
df["month"] = df["date"].dt.month
df["year"] = df["date"].dt.year

# Groupby –ø—Ä–∏–∑–Ω–∞–∫–∏
agg = df.groupby("category")["num_papers"].agg(["mean", "std"])
df = df.merge(agg, on="category", how="left")

X = df[["month", "year", "mean", "std"]]
y = df["num_papers"]

model = lgb.LGBMRegressor()
model.fit(X, y)
```

### üê∂ –ó–∞–¥–∞—á–∞: –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ—Ä–≥–∏ (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, PCA)
**–ú–µ—Ç–æ–¥:** PCA + –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```python
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

X_corgis = images.reshape(images.shape[0], -1)
X_pca = PCA(n_components=2).fit_transform(X_corgis)
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title("Corgi Clusters")
plt.show()
```

### üîÑ –ó–∞–¥–∞—á–∞: –ø–æ–≤–æ—Ä–æ—Ç MNIST
**–ú–µ—Ç–æ–¥:** PCA –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö + –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, GradientBoosting)
```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.decomposition import PCA

X_mnist = images.reshape(images.shape[0], -1)
X_pca = PCA(n_components=40).fit_transform(X_mnist)
model = GradientBoostingClassifier()
model.fit(X_pca, y)
```

### üí¨ –ó–∞–¥–∞—á–∞: –º–∞—Ç—á–∏–Ω–≥ –≤—Ä–∞—á–µ–±–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –∏ —Å–∞–º–º–∞—Ä–∏
**–ú–µ—Ç–æ–¥:** TF-IDF + cosine similarity
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

vec = TfidfVectorizer(max_features=500)
X = vec.fit_transform(df["report"])
Y = vec.transform(df["summary"])

similarities = cosine_similarity(X, Y)
preds = similarities.argmax(axis=1)
```

## üß© –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

### TF-IDF (–¥–ª—è —Ç–µ–∫—Å—Ç–∞)
```python
from sklearn.feature_extraction.text import TfidfVectorizer
texts = ["AI is the future", "Machine learning is a subset of AI"]
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(texts)
```

### CountVectorizer (–ø—Ä–æ—Å—Ç–æ–π bag-of-words)
```python
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)
```

### PCA (–¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)
```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())
```

### GroupBy –ø—Ä–∏–∑–Ω–∞–∫–∏
```python
df = pd.DataFrame({"category": ["cs", "cs", "physics"], "length": [100, 150, 200]})
grouped = df.groupby("category")["length"].agg(["mean", "std"])
df = df.merge(grouped, on="category", how="left")
```

### –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (flatten)
```python
X_images = images.reshape(images.shape[0], -1)
X_pca = PCA(n_components=50).fit_transform(X_images)
```

## üì• –†–∞–±–æ—Ç–∞ —Å pandas –∏ NumPy

### –ò–º–ø–æ—Ä—Ç –∏ –±–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
```python
df = pd.read_csv("data.csv")
df["mean"] = df.mean(axis=1)
df["sum"] = df.sum(axis=1)
```

### NumPy
```python
import numpy as np
np.mean(arr), np.std(arr), np.argmax(arr), np.argsort(arr), np.unique(arr)
```

## ü§ñ –ú–æ–¥–µ–ª–∏

### Linear Regression
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```

### Logistic Regression
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=300)
model.fit(X_train, y_train)
```

### LightGBM
```python
import lightgbm as lgb
model = lgb.LGBMClassifier()
model.fit(X_train, y_train)
```

## ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è

### Train/Test Split
```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

### KFold
```python
from sklearn.model_selection import KFold
kf = KFold(n_splits=5)
for train_idx, test_idx in kf.split(X):
    model.fit(X[train_idx], y[train_idx])
    preds = model.predict(X[test_idx])
```

## üì§ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∞–±–º–∏—Ç–∞
```python
submission = pd.DataFrame({"id": df["id"], "label": predictions})
submission.to_csv("submission.csv", index=False)
```

## üèÅ –§–∏–Ω–∞–ª: –ò—Å–ø–æ–ª—å–∑—É–π –º–∏–Ω–∏–º—É–º ‚Äî –ø–æ–±–µ–∂–¥–∞–π –º–∞–∫—Å–∏–º—É–º–æ–º üí™
