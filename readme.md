# ‚úÖ MACHINE LEARNING OLYMPIAD CHEATSHEET (with code snippets, beginner-friendly)

### üåê USEFUL RESOURCES DURING COMPETITION (EXTENDED)
| –ù–∞–∑–≤–∞–Ω–∏–µ | –°—Å—ã–ª–∫–∞ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å / –ß—Ç–æ –∏—Å–∫–∞—Ç—å |
|----------|--------|-------------------------------|
| Python Docs | https://docs.python.org/3/ | –õ—é–±–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–æ–¥—É–ª—å, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Ç–∏–ø—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: `str.split`, `itertools`, `zip`) |
| NumPy | https://numpy.org/doc/ | –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏, –ª–∏–Ω–∞–ª–≥, reshape, broadcasting (–Ω–∞–ø—Ä–∏–º–µ—Ä: `np.mean`, `np.linalg.inv`) |
| Pandas | https://pandas.pydata.org/docs/ | –†–∞–±–æ—Ç–∞ —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∞ datetime (–Ω–∞–ø—Ä–∏–º–µ—Ä: `df.groupby`, `df.merge`) |
| SciPy | https://docs.scipy.org/doc/scipy/ | –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (`scipy.stats`), –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è, –º–∞—Ç—Ä–∏—á–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: `ttest_ind`, `sparse`) |
| Matplotlib | https://matplotlib.org/stable/contents.html | –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤: `plt.plot`, `plt.hist`, `plt.imshow` –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è |
| Seaborn | https://seaborn.pydata.org/ | –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: `sns.heatmap`, `sns.boxplot`, `sns.pairplot` |
| Scikit-learn | https://scikit-learn.org/stable/ | –í—Å–µ ML –º–æ–¥–µ–ª–∏, –º–µ—Ç—Ä–∏–∫–∏ (`accuracy_score`, `f1_score`), –ø–∞–π–ø–ª–∞–π–Ω—ã, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã |
| XGBoost | https://xgboost.readthedocs.io/en/stable/ | –í—Å–µ –ø—Ä–æ `XGBClassifier`, DMatrix, `cv`, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã `eta`, `max_depth`, `early_stopping_rounds` |
| LightGBM | https://lightgbm.readthedocs.io/en/stable/ | –ë—ã—Å—Ç—Ä–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ XGBoost, —á–∞—Å—Ç–æ –ª—É—á—à–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π |
| CatBoost | https://catboost.ai/en/docs/ | –ë—É—Å—Ç–∏–Ω–≥ –±–µ–∑ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π: —Å–º. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã `cat_features`, `loss_function` |
| PyTorch | https://pytorch.org/docs/stable/index.html | –í—Å–µ –ø—Ä–æ `nn.Module`, `optim`, `autograd`, `torch.utils.data`, `device` |
| TensorFlow | https://www.tensorflow.org/api_docs | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ PyTorch, —Å–º. `tf.keras.models`, `tf.data`, `@tf.function`, TPU support |
| Kaggle Docs | https://www.kaggle.com/docs | –ö–∞–∫ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Ñ–∞–π–ª—ã (`/kaggle/input`), –∫–∞–∫ —Å–∞–±–º–∏—Ç–∏—Ç—å (`submission.csv`) –∏ –∫–∞–∫ –ø–∏—Å–∞—Ç—å notebook |
| Towards Data Science | https://towardsdatascience.com/ | –ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–¥–µ—é/–ø—Ä–æ–µ–∫—Ç (—á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–ª—è NLP, CV, feature engineering) |
| Analytics Vidhya | https://www.analyticsvidhya.com/ | –ó–∞–¥–∞—á–∏ –ø–æ structured data, –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º, pipeline, EDA |
| Distill.pub | https://distill.pub/ | –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ attention, –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, backpropagation ‚Äî –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π |
| ML Cheatsheet | https://ml-cheatsheet.readthedocs.io/ | –§–æ—Ä–º—É–ª—ã –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º, —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, loss-—Ñ—É–Ω–∫—Ü–∏–∏, –ø–∞–π–ø–ª–∞–π–Ω |
| Sebastian Raschka Blog | https://sebastianraschka.com/ | –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è ML —Ç–µ–æ—Ä–∏—è + –∫–æ–¥: softmax, crossval, multinomialNB –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ |
| KDnuggets | https://www.kdnuggets.com/ | –ë—ã—Å—Ç—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏, —Ä–∞–∑–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –∫–µ–π—Å–æ–≤ |
| GitHub | https://github.com/ | –ù–∞–π—Ç–∏ –ø—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏: `resnet pytorch`, `bert classifier`, `xgboost cv` |
| Kaggle | https://www.kaggle.com/ | –ü–æ–∏—Å–∫ kernel —Å –ø–æ—Ö–æ–∂–µ–π –∑–∞–¥–∞—á–µ–π, baseline —Ä–µ—à–µ–Ω–∏—è, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã submission –∏ leaderboard tips |

> üìå Tip: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CTRL+F –Ω–∞ —ç—Ç–∏—Ö —Å–∞–π—Ç–∞—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω—É–∂–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ –ø—Ä–∏–º–µ—Ä–∞. –ù–∞–ø—Ä–∏–º–µ—Ä: `site:scikit-learn.org train_test_split` –≤ Google.

...

# ‚úÖ MACHINE LEARNING OLYMPIAD CHEATSHEET (with code snippets, beginner-friendly)

---

### ‚öôÔ∏è Additional Preprocessing Techniques (For Any Case)

```python
from sklearn.preprocessing import MinMaxScaler, RobustScaler, LabelEncoder, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_classif

# Missing values
imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent'
X_imputed = imputer.fit_transform(X)

# Scaling alternatives
X_minmax = MinMaxScaler().fit_transform(X)
X_robust = RobustScaler().fit_transform(X)  # Works better with outliers

# Encode target if it's not numeric
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Gaussianize skewed features
X_powered = PowerTransformer().fit_transform(X)  # Box-Cox or Yeo-Johnson

# Feature selection
X_kbest = SelectKBest(f_classif, k=10).fit_transform(X, y)  # Use with caution
```

---

### üß† Alternative Models for Tabular Data

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# Try multiple classifiers quickly
models = [
    LogisticRegression(max_iter=1000),
    RandomForestClassifier(n_estimators=100),
    GradientBoostingClassifier(),
    LGBMClassifier(),
    CatBoostClassifier(verbose=0)
]

for model in models:
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    print(type(model).__name__, f1_score(y_valid, preds))
```

---

### üîÅ Model Ensembling & Stacking (Classic Way)

```python
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# Hard/soft voting
voting = VotingClassifier([
    ('lr', LogisticRegression(max_iter=1000)),
    ('rf', RandomForestClassifier()),
    ('svc', SVC(probability=True))
], voting='soft')

voting.fit(X_train, y_train)
print('Voting F1:', f1_score(y_valid, voting.predict(X_valid)))

# Stacking (uses meta-model)
stack = StackingClassifier(
    estimators=[('rf', RandomForestClassifier()), ('nb', GaussianNB())],
    final_estimator=LogisticRegression()
)
stack.fit(X_train, y_train)
```

---

### üóÇ Use These Depending On:
- **Heavy outliers?** ‚Üí `RobustScaler`, `IsolationForest`, `PowerTransformer`
- **Mixed dtypes?** ‚Üí `CatBoost`, `ColumnTransformer`, `OneHot + Num`
- **Small data?** ‚Üí `TF-IDF`, `LogisticRegression`, no deep models
- **Many classes?** ‚Üí use `objective='multi:softprob'` or `multi_class='multinomial'`
- **Time-series?** ‚Üí sort by time, no shuffling, use lag features, `TimeSeriesSplit`
- **Sparse inputs?** ‚Üí `SGDClassifier`, `MultinomialNB`
- **Slow training?** ‚Üí `hist` tree method (XGBoost), `max_bins`, or downsampling


### üìå 1. DATA PREPROCESSING PIPELINE (EXPLAINED)
```python
# Step-by-step data preparation for structured/tabular data
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE

# 1. Load dataset
df = pd.read_csv('data.csv')             # Your dataset must contain a 'target' column
X = df.drop('target', axis=1)            # Features only
y = df['target']                         # Labels

# 2. Remove multivariate outliers
iso = IsolationForest(contamination=0.01, random_state=42)
outlier_mask = iso.fit_predict(X) == 1
X, y = X[outlier_mask], y[outlier_mask]  # Keep normal rows only

# 3. Encode categorical features
cat_cols = X.select_dtypes(['object', 'category']).columns
ohe = OneHotEncoder(sparse=False, drop='first')
X_cat = pd.DataFrame(ohe.fit_transform(X[cat_cols]), index=X.index)
X = pd.concat([X.drop(cat_cols, axis=1), X_cat], axis=1)

# 4. Scale numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Optionally reduce dimensions (e.g., to speed up training)
pca = PCA(n_components=0.95)  # Keeps 95% of variance
X_reduced = pca.fit_transform(X_scaled)

# 6. Handle class imbalance (e.g., in binary classification)
smote = SMOTE(random_state=42)
X_bal, y_bal = smote.fit_resample(X_reduced, y)
```

### üìå 2. HYPERPARAMETER TUNING FOR XGBOOST (EXPLAINED)

> üéØ **–¶–µ–ª—å:** –Ω–∞–π—Ç–∏ –Ω–∞–∏–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ XGBoost, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –µ—ë –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å.
> üì¶ **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ–≥–¥–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –¥–∞—é—Ç –Ω—É–∂–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏.
> üß† **–ü–æ—á–µ–º—É Optuna:** –æ–Ω –±—ã—Å—Ç—Ä–µ–µ –∏ —É–º–Ω–µ–µ, —á–µ–º –ø–µ—Ä–µ–±–æ—Ä (grid search), —Ç.–∫. –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é.

üß© **–¢–∏–ø–æ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `eta` ‚Äî learning rate: –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- `max_depth` ‚Äî –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞: –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- `subsample` ‚Äî –¥–æ–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞: —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —à—É–º
- `colsample_bytree` ‚Äî –¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: —Å–Ω–∏–∂–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –¥–µ—Ä–µ–≤—å–µ–≤
- `min_child_weight` ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞ –≤–µ—Å–∞ —É–∑–ª–∞

üí° **–ö–∞–∫ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å:**
- –ú–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å `eval_metric` –Ω–∞ `logloss`, `error`, `rmse` –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
- –î–ª—è multi-class: `objective='multi:softprob', num_class=N`

```python
# Use Optuna to find optimal model parameters
import optuna
import xgboost as xgb
from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(X_bal, y_bal, test_size=0.2, random_state=42)

def objective(trial):
    # Define hyperparameter search space
    param = {
        'verbosity': 0,
        'objective': 'binary:logistic',
        'tree_method': 'hist',
        'eval_metric': 'auc',
        'eta': trial.suggest_loguniform('eta', 1e-3, 0.3),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
    }
    dtrain = xgb.DMatrix(X_train, label=y_train)
    cv = xgb.cv(param, dtrain, nfold=3, seed=42, num_boost_round=100, early_stopping_rounds=10)
    return cv['test-auc-mean'].max()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30)

# Train final model with best parameters
best_params = study.best_params
model = xgb.XGBClassifier(**best_params, use_label_encoder=False)
model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=False)
```

### üìå 3. TEXT CLASSIFICATION PIPELINES (WITH OPTIONS)

> üéØ **–¶–µ–ª—å:** –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä: —Å–ø–∞–º, —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, –∫–∞—Ç–µ–≥–æ—Ä–∏—é).
> üìò **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –∫–æ–≥–¥–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏, –∏ –∏—Ö –Ω—É–∂–Ω–æ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤ –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å.

üß© **–î–≤–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞:**
- A: TF-IDF (–ª–µ–≥–∫–æ, –±—ã—Å—Ç—Ä–æ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ)
- B: BERT embedding (–≥–ª—É–±–æ–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞ —Ç–µ–∫—Å—Ç–∞)

üí° **TF-IDF –ø–æ–¥–æ–π–¥—ë—Ç –¥–ª—è:** –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ–≥–¥–∞ –≤–∞–∂–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
üí° **BERT embeddings –ø–æ–¥–æ–π–¥—ë—Ç –¥–ª—è:** –¥–ª–∏–Ω–Ω—ã—Ö, —Å–ª–æ–∂–Ω—ã—Ö —Ñ—Ä–∞–∑; –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª—É—á—à–µ

üõ† **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ HuggingFace –∏–ª–∏ fine-tune –Ω–∞–ø—Ä—è–º—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä: DistilBERT + –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä)
```python
# Approach A: TF-IDF + Logistic Regression
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

texts = [...]   # List of raw strings
labels = [...]  # Corresponding labels

tfidf_lr = Pipeline([
    ('tfidf', TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1,2), max_df=0.8, min_df=5)),
    ('clf', LogisticRegression(max_iter=1000))
])
tfidf_lr.fit(texts, labels)
```

```python
# Approach B: BERT Embeddings + Random Forest
from transformers import AutoTokenizer, AutoModel
from sklearn.ensemble import RandomForestClassifier
import torch
import numpy as np

# Load pretrained tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
bert = AutoModel.from_pretrained('bert-base-uncased').eval()

def get_cls_emb(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        out = bert(**inputs)
    return out.last_hidden_state[:,0].squeeze().numpy()

X_emb = np.vstack([get_cls_emb(t) for t in texts])
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_emb, labels)
```

### ‚úÖ HUGGINGFACE MODELS USABLE ON KAGGLE WITHOUT AUTH
These models are small/medium and commonly cached on Kaggle:
- 'bert-base-uncased'
- 'distilbert-base-uncased'
- 'roberta-base'
- 'distilroberta-base'
- 'albert-base-v2'
- 'sentence-transformers/all-MiniLM-L6-v2'
- 'google/electra-small-discriminator'
- 'cardiffnlp/twitter-roberta-base-sentiment'

```python
from transformers import AutoTokenizer, AutoModel
model = AutoModel.from_pretrained('distilbert-base-uncased')
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
```

### üìå 4. CROSS-VALIDATION PIPELINE (EXPLAINED)

> üéØ **–¶–µ–ª—å:** –ø–æ–ª—É—á–∏—Ç—å —á–µ—Å—Ç–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∑–∞ —Å—á—ë—Ç –µ—ë —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞—Å—Ç—è—Ö –¥–∞–Ω–Ω—ã—Ö.
> üß™ **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ –∏–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.

üß© **–ü–æ—á–µ–º—É StratifiedKFold:** –æ–Ω —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–∞–∂–¥–æ–º —Å–ø–ª–∏—Ç–µ (–≤–∞–∂–Ω–æ –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ –∫–ª–∞—Å—Å–æ–≤).

üí° **–°–æ–≤–µ—Ç—ã:**
- –ò—Å–ø–æ–ª—å–∑—É–π `GroupKFold`, –µ—Å–ª–∏ –µ—Å—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, user_id)
- –î–ª—è time-series: `TimeSeriesSplit`, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–µ–∫ –±—É–¥—É—â–µ–≥–æ
- –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å: `roc_auc_score`, `log_loss`, `accuracy_score`
```python
# Use StratifiedKFold to evaluate model performance reliably
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from xgboost import XGBClassifier

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
all_scores = []

for train_idx, val_idx in cv.split(X_bal, y_bal):
    X_tr, X_val = X_bal[train_idx], X_bal[val_idx]
    y_tr, y_val = y_bal[train_idx], y_bal[val_idx]

    model = XGBClassifier(**best_params)
    model.fit(X_tr, y_tr)
    preds = model.predict(X_val)
    score = f1_score(y_val, preds)
    all_scores.append(score)

print("Mean CV F1:", np.mean(all_scores))
```

### üìå 5. COMPUTER VISION (CV) SNIPPETS (FOR CLASSIFICATION)

> üéØ **–¶–µ–ª—å:** –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–∏ –ø–æ–º–æ—â–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π.
> üìò **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –ø—Ä–∏ –∑–∞–¥–∞—á–∞—Ö —Ç–∏–ø–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∫–ª–∞—Å—Å–æ–≤, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.

üß© **–ü—Ä–∏–º–µ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞:**
1. Torchvision –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (ResNet, EfficientNet)
2. ImageFolder ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–µ—Ç–∫–∞–º–∏ –ø–æ –ø–∞–ø–∫–∞–º
3. Albumentations ‚Äî –¥–µ–ª–∞–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å

üí° **–ß—Ç–æ –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å:**
- –°–ª–æ–π `model.fc` –º–æ–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ —Å–≤–æ—ë –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
- –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å `Blur`, `RandomCrop`, `ShiftScaleRotate`
- –õ–æ—Å—Å: –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ `nn.BCEWithLogitsLoss()` –¥–ª—è multilabel
```python
# Torchvision pretrained model + custom classifier
import torch
import torchvision
import torchvision.transforms as T
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torch import nn, optim

transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

train_ds = ImageFolder("/path/train", transform=transform)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)

model = torchvision.models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, len(train_ds.classes))  # Set to num_classes

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Train loop (simple version)
for epoch in range(5):
    model.train()
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

```python
# Albumentations for image augmentation
import albumentations as A
from albumentations.pytorch import ToTensorV2

transform = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.Normalize(),
    ToTensorV2()
])
```

### üîÅ FULL TRAINING PIPELINE TEMPLATE (START TO FINISH)
```python
# 1. Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

# 2. Load dataset
df = pd.read_csv("your_data.csv")
X = df.drop("target", axis=1)
y = df["target"]

# 3. Clean and preprocess data
numeric_cols = X.select_dtypes(include=np.number).columns
cat_cols = X.select_dtypes(include='object').columns

num_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])

cat_pipe = Pipeline([
    ("encoder", OneHotEncoder(drop='first', sparse=False))
])

X_num = pd.DataFrame(num_pipe.fit_transform(X[numeric_cols]), columns=numeric_cols)
X_cat = pd.DataFrame(cat_pipe.fit_transform(X[cat_cols]), index=X.index)

X_final = pd.concat([X_num, X_cat], axis=1)

# 4. Dimensionality reduction (optional)
#from sklearn.decomposition import PCA
#pca = PCA(n_components=0.95)
#X_final = pca.fit_transform(X_final)

# 5. Balance classes (if needed)
sm = SMOTE(random_state=42)
X_bal, y_bal = sm.fit_resample(X_final, y)

# 6. Split train/validation
X_train, X_valid, y_train, y_valid = train_test_split(X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42)

# 7. Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 8. Validate
preds = model.predict(X_valid)
print("F1 Score:", f1_score(y_valid, preds))

# 9. (Optional) Cross-validation
cv = StratifiedKFold(n_splits=5)
scores = []
for train_idx, val_idx in cv.split(X_bal, y_bal):
    model.fit(X_bal[train_idx], y_bal[train_idx])
    preds = model.predict(X_bal[val_idx])
    scores.append(f1_score(y_bal[val_idx], preds))

print("CV F1 avg:", np.mean(scores))
```

# End of Cheatsheet
